{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b672284-b9d0-4f68-a4bd-6186414bb980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1\n",
      "Scraping Page 2\n",
      "Scraping Page 3\n",
      "Scraping Page 4\n",
      "Scraping Page 5\n",
      "Scraping Page 6\n",
      "Scraping Page 7\n",
      "Scraping Page 8\n",
      "Scraping Page 9\n",
      "Scraping Page 10\n",
      "Maximum number of pages, 10, reached!\n",
      "Scraping complete, found 100 total quotes\n",
      "Data has been saved to scraped_quotes.csv.\n"
     ]
    }
   ],
   "source": [
    "# Python Website Scraper, June 2025\n",
    "# Scraper uses Selenium, and is set to the default website of \"http://quotes.toscrape.com/\"\n",
    "# Current elements the scraper is tuned for are \"author\" and \"quotes\"\n",
    "# final data is packaged in a neat little .csv file to export to Excel or Google Sheets\n",
    "\n",
    "\n",
    "#Importing Tools\n",
    "import time\n",
    "import csv #tool for csv files\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException #Handles end of pages\n",
    "from webdriver_manager.chrome import  ChromeDriverManager\n",
    "\n",
    "# Set up Browser\n",
    "driver_service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=driver_service)\n",
    "wait = WebDriverWait(driver, 10)\n",
    "driver.get(\"http://quotes.toscrape.com/\")\n",
    "\n",
    "# Preparation for Data Collection\n",
    "all_quotes = []\n",
    "page_number = 1\n",
    "max_pages = 10 \n",
    "\n",
    "#Scrape Loop\n",
    "while True:\n",
    "    if page_number > max_pages:\n",
    "        print(f\"Maximum number of pages, {max_pages}, reached!\")\n",
    "        break\n",
    "    print(f\"Scraping Page {page_number}\")\n",
    "    #Find all quote elements on the page\n",
    "    try: \n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"quote\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load. Ending scrape.\")\n",
    "        break\n",
    "    \n",
    "    quote_elements = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "    \n",
    "    #Loop to locate quote elements\n",
    "    for quote_element in quote_elements:\n",
    "        #scrape quote text & author\n",
    "        text = quote_element.find_element(By.CLASS_NAME, \"text\").text\n",
    "        author = quote_element.find_element(By.CLASS_NAME, \"author\").text\n",
    "    \n",
    "        #store data in dictonary\n",
    "        quote_data = {\n",
    "            \"author\": author,\n",
    "            \"quote\": text,\n",
    "        }\n",
    "        #add to list\n",
    "        all_quotes.append(quote_data)\n",
    "\n",
    "#Finding and clicking the next button to continue scraping the next page\n",
    "    try:\n",
    "        # Website labels the next button with the class, \"next\"\n",
    "        next_button = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"next\")))\n",
    "\n",
    "        #next_button.click()\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        page_number += 1 # Increment our page counter\n",
    "        \n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        # Runs if the try block fails, allowing the program to end once all pages have been scraped\n",
    "        print(\"No more pages to scrape. Finished.\")\n",
    "        break # finalize the scrape\n",
    "\n",
    "#Saving the data to a CSV file\n",
    "print(f\"Scraping complete, found {len(all_quotes)} total quotes\")\n",
    "\n",
    "#Defining the file\n",
    "csv_file_name = \"scraped_quotes.csv\"\n",
    "\n",
    "# Opens the file in write mode, \"newline =\" is the standard for csv files\n",
    "with open(csv_file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "\n",
    "    #Define column headers for the file by matching it ot keys from \"quote_data\" dict\n",
    "    fieldnames = [\"author\", \"quote\"]\n",
    "\n",
    "    #Creates a writer object thatknows how to write rows to our file\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "    #Write the first row, that will be a header\n",
    "    writer.writeheader()\n",
    "\n",
    "    #write the rest of our data\n",
    "    writer.writerows(all_quotes)\n",
    "\n",
    "print(f\"Data has been saved to {csv_file_name}.\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adc445-690a-4a3a-8ddc-2be29d1acc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6bc12-b013-4025-b1b7-d5ab626861f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
